{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09-01-attention-과제-2.ipynb","provenance":[],"collapsed_sections":["6dfJPT-2XMTB","JHkHg6XAXoyK","tPfBcykRf6TH","XwriCkq_R1Lc","D-bjMMvkBRLU","kTgzpx2Wl6uv","wZSehzviqJW3","x0ZAlao5qJW4","ByyA-3bbqJW6","2lWovHbAqJW7"],"authorship_tag":"ABX9TyPVCJXPcyndW8mQ4uwnNE5K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6dfJPT-2XMTB"},"source":["# Install"]},{"cell_type":"code","metadata":{"id":"a193aGJWVaqb"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JHkHg6XAXoyK"},"source":["# Evn"]},{"cell_type":"code","metadata":{"id":"WkYXFwcBXJDG"},"source":["import os\n","import random\n","import shutil\n","import json\n","import zipfile\n","import math\n","import copy\n","import collections\n","import re\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import sentencepiece as spm\n","import tensorflow as tf\n","import tensorflow.keras.backend as KK\n","\n","from tqdm.notebook import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvjyruUlXtlR"},"source":["# random seed initialize\n","random_seed = 1234\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","tf.random.set_seed(random_seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BC3fXkhdYcYt"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVRdxYReYeQj"},"source":["# google drive mount\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byCIiLJBbFHh"},"source":["# data dir\n","data_dir = '/content/drive/MyDrive/Data/nlp'\n","os.listdir(data_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Ru56YS3-SME"},"source":["# songys chatbot dir\n","songys_dir = os.path.join(data_dir, 'songys')\n","if not os.path.exists(songys_dir):\n","    os.makedirs(songys_dir)\n","os.listdir(songys_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tPfBcykRf6TH"},"source":["# Attention"]},{"cell_type":"code","metadata":{"id":"CICSJu8iXG2z"},"source":["def get_pad_mask(tokens, i_pad=0):\n","    \"\"\"\n","    pad mask 계산하는 함수\n","    :param tokens: tokens (bs, n_seq)\n","    :param i_pad: id of pad\n","    :return mask: pad mask (pad: 1, other: 0)\n","    \"\"\"\n","    # 0인 부분 확인\n","    mask = tf.math.equal(tokens, i_pad)\n","    # boolean -> float 32\n","    mask = tf.cast(mask, tf.float32)\n","    # expand dimension for n_seq\n","    mask = tf.expand_dims(mask, axis=1)\n","    return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejLFFOfDXTCM"},"source":["class ScaleDotProductAttention(tf.keras.layers.Layer):\n","    \"\"\"\n","    Scale Dot Product Attention Class\n","    \"\"\"\n","    def __init__(self, name=\"scale_dot_product_attention\"):\n","        \"\"\"\n","        생성자\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: Q, K, V, attn_mask tuple\n","        :return attn_out: attention 실행 결과\n","        \"\"\"\n","        Q, K, V, attn_mask = inputs\n","        # matmul Q, K (transpose_b=True)\n","        attn_score = tf.matmul(Q, K, transpose_b=True)\n","        # get scale = d_model ** 0.5\n","        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n","        # divide by scale\n","        attn_scale = tf.math.divide(attn_score, scale)\n","        # do mask (subtract 1e-9 for masked value)\n","        attn_scale -= 1.e9 * attn_mask\n","        # calculate attention prob\n","        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n","        # weighted sum of V\n","        attn_out = tf.matmul(attn_prob, V)\n","        return attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XwriCkq_R1Lc"},"source":["# Vocabulary & config"]},{"cell_type":"code","metadata":{"id":"2H0BLydCb7lg"},"source":["# vocab loading\n","vocab = spm.SentencePieceProcessor()\n","vocab.load(os.path.join(data_dir, 'ko_32000.model'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ETZ19flTRmt"},"source":["n_vocab = len(vocab)  # number of vocabulary\n","n_enc_seq = 32  # number of sequence 1\n","n_dec_seq = 40  # number of sequence 2\n","d_model = 256  # dimension of model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D-bjMMvkBRLU"},"source":["# 모델링"]},{"cell_type":"code","metadata":{"id":"in7q0WMfm64D"},"source":["def build_model(n_vocab, d_model, n_enc_seq, n_dec_seq):\n","    \"\"\"\n","    문장 유사도 비교 모델\n","    :param n_vocab: vocabulary 단어 수\n","    :param d_model: 단어를 의미하는 벡터의 차원 수\n","    :param n_enc_seq: encoder 문장 길이 (단어 수)\n","    :param n_dec_seq: decoder 문장 길이 (단어 수)\n","    \"\"\"\n","    inputs_enc = tf.keras.layers.Input((n_enc_seq,))  # (bs, n_enc_seq)\n","    inputs_dec = tf.keras.layers.Input((n_dec_seq,))  # (bs, n_dec_seq)\n","    # pad mask 생성\n","    ###############################################\n","    ###############################################\n","    # 입력 단어를 vector로 변환\n","    embedding = tf.keras.layers.Embedding(n_vocab, d_model)\n","    hidden_enc = embedding(inputs_enc)  # (bs, n_enc_seq, d_model)\n","    hidden_dec = embedding(inputs_dec)  # (bs, n_dec_seq, d_model)\n","    # encoder LSTM\n","    fw_cell = tf.keras.layers.LSTM(units=d_model, return_sequences=True, return_state=True)\n","    bw_cell = tf.keras.layers.LSTM(units=d_model, go_backwards=True, return_sequences=True, return_state=True)\n","    lstm_enc = tf.keras.layers.Bidirectional(fw_cell, backward_layer=bw_cell)\n","    hidden_enc, fw_h, fw_c, bw_h, bw_c = lstm_enc(hidden_enc)  # (bs, n_enc_seq, d_model * 2), (bs, d_model), (bs, d_model), (bs, d_model), (bs, d_model)\n","    # concat hidden & cell\n","    s_h = tf.concat([fw_h, bw_h], axis=-1)  # (bs, d_model * 2)\n","    s_c = tf.concat([fw_c, bw_c], axis=-1)  # (bs, d_model * 2)\n","    # decoder LSTM\n","    lstm_dec = tf.keras.layers.LSTM(units=d_model * 2, return_sequences=True)\n","    hidden_dec = lstm_dec(hidden_dec, initial_state=[s_h, s_c])  # (bs, n_dec_seq, d_model)\n","    # ScaleDotProductAttention 실행\n","    ##############################################\n","    ##############################################\n","    # concat output\n","    dec_out = tf.concat([attn_out, hidden_dec], axis=-1)\n","    # 다음단어 예측\n","    dense_out = tf.keras.layers.Dense(units=n_vocab, activation=tf.nn.softmax)\n","    outputs = dense_out(dec_out)  # (bs, n_dec_seq, n_vocab)\n","    # 학습할 모델 선언\n","    model = tf.keras.Model(inputs=(inputs_enc, inputs_dec), outputs=outputs)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H8Bkh2pYB8zp"},"source":["# 모델 생성\n","model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n","# 모델 내용 그래프 출력\n","tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTgzpx2Wl6uv"},"source":["# Loss & Acc"]},{"cell_type":"code","metadata":{"id":"hQRKhodYtQHr"},"source":["def lm_loss(y_true, y_pred):\n","    \"\"\"\n","    pad 부분을 제외하고 loss를 계산하는 함수\n","    :param y_true: 정답\n","    :param y_pred: 예측 값\n","    :retrun loss: pad 부분이 제외된 loss 값\n","    \"\"\"\n","    # loss = sparse_entropy = tf.keras.losses.SparseCategoricalCrossentropy()(y_true, y_pred)\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n","    mask = tf.not_equal(y_true, 0)\n","    mask = tf.cast(mask, tf.float32)\n","    # print(mask)\n","    loss *= mask\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQqs7xzA3N3Q"},"source":["def lm_acc(y_true, y_pred):\n","    \"\"\"\n","    pad 부분을 제외하고 accuracy를 계산하는 함수\n","    :param y_true: 정답\n","    :param y_pred: 예측 값\n","    :retrun loss: pad 부분이 제외된 accuracy 값\n","    \"\"\"\n","    y_true = tf.cast(y_true, tf.float32)\n","    # print(y_true)\n","    y_pred_class = tf.cast(tf.argmax(y_pred, axis=-1), tf.float32)\n","    # print(y_pred_class)\n","    matches = tf.cast(tf.equal(y_true, y_pred_class), tf.float32)\n","    # print(matches)\n","    mask = tf.not_equal(y_true, 0)\n","    mask = tf.cast(mask, tf.float32)\n","    # print(mask)\n","    matches *= mask\n","    # print(matches)\n","    # accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(tf.ones_like(matches)), 1)\n","    accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(mask), 1)\n","    return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wZSehzviqJW3"},"source":["# All Data Project"]},{"cell_type":"markdown","metadata":{"id":"x0ZAlao5qJW4"},"source":["## Data\n"]},{"cell_type":"code","metadata":{"id":"0B-0r3bjqJW4"},"source":["df_train = pd.read_csv(os.path.join(songys_dir, 'ChatbotData.csv'))\n","print(len(df_train))\n","df_train = df_train.dropna()\n","print(len(df_train))\n","df_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQkIYiRSqJW5"},"source":["def load_data(df, n_enc_seq, n_dec_seq):\n","    \"\"\"\n","    Quora 학습 데이터 생성\n","    :param df: data frame\n","    :param n_enc_seq: number of encoder sequence\n","    :param n_dec_seq: number of decoder sequence\n","    :return enc_inputs: encoder input data\n","    :return dec_inputs: decoder input data\n","    :return labels: label data\n","    \"\"\"\n","    n_enc_max = n_enc_seq\n","    n_dec_max = n_dec_seq - 1\n","    enc_inputs = np.zeros((len(df), n_enc_seq)).astype(np.int32)\n","    dec_inputs = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n","    labels = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n","    index = 0\n","    for i, row in tqdm(df.iterrows(), total=len(df)):\n","        # tokens 저장\n","        Q = row['Q']\n","        A = row['A']\n","\n","        tokens_q = vocab.encode_as_pieces(Q)\n","        tokens_a = vocab.encode_as_pieces(A)\n","\n","        tokens_ids_q = vocab.encode_as_ids(Q)[:n_enc_max]\n","        tokens_ids_a = vocab.encode_as_ids(A)[:n_dec_max]\n","\n","        tokens_dec_in = [vocab.bos_id()] + tokens_ids_a\n","        tokens_dec_out = tokens_ids_a + [vocab.eos_id()]\n","\n","        tokens_ids_q += [0] * (n_enc_seq - len(tokens_ids_q))\n","        tokens_dec_in += [0] * (n_dec_seq - len(tokens_dec_in))\n","        tokens_dec_out += [0] * (n_dec_seq - len(tokens_dec_out))\n","\n","        enc_inputs[index] = tokens_ids_q\n","        dec_inputs[index] = tokens_dec_in\n","        labels[index] = tokens_dec_out\n","        index += 1\n","    return enc_inputs, dec_inputs, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FguUxO5xqJW5"},"source":["# train data 생성\n","train_enc_inputs, train_dec_inputs, train_labels = load_data(df_train, n_enc_seq, n_dec_seq)\n","train_enc_inputs, train_dec_inputs, train_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ByyA-3bbqJW6"},"source":["## 학습"]},{"cell_type":"code","metadata":{"id":"Li7XjkxHqJW6"},"source":["# 모델 생성\n","model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n","# 모델 내용 그래프 출력\n","tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yRh1IFPCqJW6"},"source":["# 모델 loss, optimizer, metric 정의\n","model.compile(loss=lm_loss, optimizer='adam', metrics=[lm_acc])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5CzpAnaqJW6"},"source":["# early stopping\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='lm_acc', patience=5)\n","# save weights callback\n","save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(songys_dir, 'lstm_scale_dot.hdf5'),\n","                                                  monitor='lm_acc',\n","                                                  verbose=1,\n","                                                  save_best_only=True,\n","                                                  mode=\"max\",\n","                                                  save_freq=\"epoch\",\n","                                                  save_weights_only=True)\n","# csv logger\n","csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(songys_dir, 'lstm_scale_dot.csv'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9q0mkUKuqJW6"},"source":["# 모델 학습\n","history = model.fit((train_enc_inputs, train_dec_inputs),\n","                    train_labels,\n","                    epochs=100,\n","                    batch_size=256,\n","                    callbacks=[early_stopping, save_weights, csv_logger])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6PMeESUYqJW6"},"source":["plt.figure(figsize=(12, 4))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], 'b-', label='loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['lm_acc'], 'g-', label='acc')\n","plt.xlabel('Epoch')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2lWovHbAqJW7"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"V11xf5bUqJW7"},"source":["# 모델 생성\n","model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n","# train weight로 초기화\n","model.load_weights(os.path.join(songys_dir, 'lstm_scale_dot.hdf5'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqMWscTSqJW7"},"source":["def do_chat(vocab, model, n_enc_seq, n_dec_seq, string):\n","    \"\"\"\n","    seq2seq chat\n","    :param vocab: vocab\n","    :param model: model\n","    :param n_enc_seq: number of enc seqence\n","    :param n_dec_seq: number of dec seqence\n","    :param string: inpust string\n","    \"\"\"\n","    # qeustion\n","    q = vocab.encode_as_pieces(string)\n","    q_id = [vocab.piece_to_id(p) for p in q][:n_enc_seq]\n","    q_id += [0] * (n_enc_seq - len(q_id))\n","    assert len(q_id) == n_enc_seq\n","\n","    # answer\n","    a_id = [vocab.bos_id()]\n","    a_id += [0] * (n_dec_seq - len(a_id))\n","    assert len(a_id) == n_dec_seq\n","\n","    # 처음부터 예측\n","    start_idx = 0\n","\n","    for _ in range(start_idx, n_dec_seq - 1):\n","        outputs = model.predict((np.array([q_id]), np.array([a_id])))\n","        prob = outputs[0][start_idx]\n","        word_id = np.argmax(prob)\n","        if word_id == vocab.eos_id():\n","            break\n","        a_id[start_idx + 1] = int(word_id)\n","        start_idx += 1\n","    predict_id = a_id[1:start_idx + 1]\n","    predict_str = vocab.decode_ids(predict_id)\n","    return predict_str"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LiEbtt7JqJW7"},"source":["while True:\n","    string = input('질문 > ')\n","    string = string.strip()\n","    if len(string) == 0:\n","        break\n","    predict_str = do_chat(vocab, model, n_enc_seq, n_dec_seq, string)\n","    print(f'답변 > {predict_str}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yo6cyrr2ehCH"},"source":[""],"execution_count":null,"outputs":[]}]}