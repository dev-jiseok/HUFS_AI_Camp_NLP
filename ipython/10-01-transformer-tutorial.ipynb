{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10-01-transformer-tutorial.ipynb","provenance":[],"collapsed_sections":["JHkHg6XAXoyK","pKEZAHeCzxRm","sDYpkXKSa0S0","BHIf3KEV09K3","DMLh4TXbGrHf","AP6eq87QHWHv","itVetPQs1D98","A_kvndtnH-gh","y6_0vG8SISFB","J5TmKt9JIXzc","uuav6G6Bh8L4","ObWGnfj6Ic_M","MH_M42l5Iezo","Z4OCTlKPIh0Z","zv8egauKIl6h","sgmPfzaBIo_V"],"authorship_tag":"ABX9TyMRjjz+lZjFGNFaRJrB41bl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JHkHg6XAXoyK"},"source":["# Evn"]},{"cell_type":"code","metadata":{"id":"WkYXFwcBXJDG","executionInfo":{"status":"ok","timestamp":1612744864553,"user_tz":-540,"elapsed":3356,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}}},"source":["import os\n","import random\n","import shutil\n","import json\n","import zipfile\n","import math\n","import copy\n","import collections\n","import re\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as KK\n","\n","from tqdm.notebook import tqdm"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvjyruUlXtlR","executionInfo":{"status":"ok","timestamp":1612744864554,"user_tz":-540,"elapsed":3352,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}}},"source":["# random seed initialize\n","random_seed = 1234\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","tf.random.set_seed(random_seed)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"BC3fXkhdYcYt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612744864555,"user_tz":-540,"elapsed":3351,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}},"outputId":"928424d4-5c7d-4344-aea7-ef0ff82bc29d"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pKEZAHeCzxRm"},"source":["# Config"]},{"cell_type":"code","metadata":{"id":"u9VJvPSx12Q8"},"source":["class Config(dict):\n","    \"\"\"\n","    json을 config 형태로 사용하기 위한 Class\n","    :param dict: config dictionary\n","    \"\"\"\n","    __getattr__ = dict.__getitem__\n","    __setattr__ = dict.__setitem__\n","\n","    @classmethod\n","    def load(cls, file):\n","        \"\"\"\n","        file에서 Config를 생성 함\n","        :param file: filename\n","        \"\"\"\n","        with open(file, 'r') as f:\n","            config = json.loads(f.read())\n","            return Config(config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Bb_Q7DhzlGp"},"source":["# config 생성\n","# d_model: model hidden dim\n","# n_head: multi head attention head number\n","# d_head: multi head attention head dim\n","# dropout: dropout rate\n","# d_ff: feed forward dim\n","# norm_eps: layernormal epsilon\n","# n_layer: layer number\n","# n_seq: sequence max number\n","# n_vocab: vocab count\n","# i_pad: vocab pad id\n","config = Config({\"d_model\": 8,\n","                 \"n_head\": 2,\n","                 \"d_head\": 4,\n","                 \"dropout\": 0.1,\n","                 \"d_ff\": 32,\n","                 \"norm_eps\": 0.001,\n","                 \"n_layer\": 6,\n","                 \"n_seq\": 16,\n","                 \"n_vocab\": 16,\n","                 \"i_pad\": 0})\n","config"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sDYpkXKSa0S0"},"source":["# Input"]},{"cell_type":"code","metadata":{"id":"bD4ou2ahas6z"},"source":["# 입력 문장\n","sentences = [\n","    ['나는 오늘 행복해', '나도 기분이 좋아'],\n","    # ['나는 오늘 기분이 좋아', '나도 매우 행복하다'],\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVax4yWKa6iG"},"source":["# 각 문장을 띄어쓰기 단위로 분할\n","words = []\n","for pair in sentences:\n","    for sentence in pair:\n","        words.extend(sentence.split())\n","\n","# 중복 단어 제거\n","words = list(dict.fromkeys(words))\n","\n","# 각 단어별 고유한 번호 부여\n","word_to_id = {'[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3}\n","for word in words:\n","    word_to_id[word] = len(word_to_id)\n","\n","# 각 숫자별 단어 부여\n","id_to_word = {_id:word for word, _id in word_to_id.items()}\n","\n","word_to_id, id_to_word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ooN0V7loa8a9"},"source":["# Question과 Answer를 숫자료\n","question_list, answer_list = [], []\n","\n","for pair in sentences:\n","    question_list.append([word_to_id[word] for word in pair[0].split()])\n","    answer_list.append([word_to_id[word] for word in pair[1].split()])\n","\n","# 학습용 입력 데이터 생성\n","train_enc_inputs, train_dec_inputs, train_labels = [], [], []\n","for question, answer in zip(question_list, answer_list):\n","    train_enc_inputs.append(question)\n","    train_dec_inputs.append([word_to_id['[BOS]']] + answer)\n","    train_labels.append(answer + [word_to_id['[EOS]']])\n","\n","# Encoder 입력의 길이를 모두 동일하게 변경 (최대길이 4)\n","for row in train_enc_inputs:\n","    row += [0] * (4 - len(row))\n","\n","# Decoder 입력의 길이를 모두 동일하게 변경 (최대길이 6)\n","for row in train_dec_inputs:\n","    row += [0] * (6 - len(row))\n","\n","# 정답의 길이를 모두 동일하게 변경 (최대길이 6)\n","for row in train_labels:\n","    row += [0] * (6 - len(row))\n","\n","# numpy array로 변환/\n","train_enc_inputs = np.array(train_enc_inputs)\n","train_dec_inputs = np.array(train_dec_inputs)\n","train_labels = np.array(train_labels)\n","\n","train_enc_inputs, train_dec_inputs, train_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5hpmht2zeTC9"},"source":["# embedding with random weight\n","embed_weight = np.random.randint(-9, 10, (config.n_vocab, config.d_model)) / 10\n","\n","embed = tf.keras.layers.Embedding(config.n_vocab, config.d_model, weights=[embed_weight])\n","embed_weight"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOj4JVDTe2Ct"},"source":["# encoder hidden\n","hidden_enc = embed(train_enc_inputs)\n","hidden_enc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJWefv-5e-wu"},"source":["# decoder hidden\n","hidden_dec = embed(train_dec_inputs)\n","hidden_dec"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BHIf3KEV09K3"},"source":["# Mask"]},{"cell_type":"markdown","metadata":{"id":"DMLh4TXbGrHf"},"source":["## PAD Mask"]},{"cell_type":"code","metadata":{"id":"ctSRBdAeG7xd"},"source":["def get_pad_mask(tokens, i_pad=0):\n","    \"\"\"\n","    pad mask 계산하는 함수\n","    :param tokens: tokens (bs, n_seq)\n","    :param i_pad: id of pad\n","    :return mask: pad mask (pad: 1, other: 0)\n","    \"\"\"\n","    # 0인 부분 확인\n","    mask = tf.math.equal(tokens, i_pad)\n","    # boolean -> float 32\n","    mask = tf.cast(mask, tf.float32)\n","    # expand dimension for n_seq\n","    mask = tf.expand_dims(mask, axis=1)\n","    return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqJIRPejHOuX"},"source":["enc_pad_mask = get_pad_mask(train_enc_inputs)\n","enc_pad_mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AP6eq87QHWHv"},"source":["## Causal Mask"]},{"cell_type":"code","metadata":{"id":"jgCB3zk-HykT"},"source":["def get_causal_mask(tokens, i_pad=0):\n","    \"\"\"\n","    causal mask 계산하는 함수\n","    :param tokens: tokens (bs, n_seq)\n","    :param i_pad: id of pad\n","    :return mask: causal and pad mask (causal or pad: 1, other: 0)\n","    \"\"\"\n","    # 개수 조회\n","    n_seq = tf.shape(tokens)[1]\n","    # make ahead mask\n","    mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n","    # expand dim for bs\n","    mask = tf.expand_dims(mask, axis=0)\n","    # get pad_mask\n","    pad_mask = get_pad_mask(tokens, i_pad)\n","    # mask all ahead_mask or pad_mask\n","    mask = tf.maximum(mask, pad_mask)\n","    return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gUsByA_eH296"},"source":["dec_causal_mask = get_causal_mask(train_dec_inputs)\n","dec_causal_mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"itVetPQs1D98"},"source":["## Mask 생성"]},{"cell_type":"code","metadata":{"id":"BRfuGqhx1Ddw"},"source":["# Encoder Self Attetnion mask\n","enc_self_mask = get_pad_mask(train_enc_inputs)\n","enc_self_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bGIvBeLy1C5P"},"source":["# Decoder Self Attetnion mask\n","dec_self_mask = get_causal_mask(train_dec_inputs)\n","dec_self_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MbU42aps1Vy3"},"source":["# Encoder-Decoder Attetnion mask\n","enc_dec_mask = get_pad_mask(train_enc_inputs)\n","enc_dec_mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A_kvndtnH-gh"},"source":["# Scaled dot product attention"]},{"cell_type":"code","metadata":{"id":"IFkp9rLd_Etz"},"source":["class ScaleDotProductAttention(tf.keras.layers.Layer):\n","    \"\"\"\n","    Scale Dot Product Attention Class\n","    \"\"\"\n","    def __init__(self, name=\"scale_dot_product_attention\"):\n","        \"\"\"\n","        생성자\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: Q, K, V, attn_mask tuple\n","        :return attn_out: attention 실행 결과\n","        \"\"\"\n","        Q, K, V, attn_mask = inputs\n","        # matmul Q, K (transpose_b=True)\n","        attn_score = tf.matmul(Q, K, transpose_b=True)\n","        # get scale = d_model ** 0.5\n","        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n","        # divide by scale\n","        attn_scale = tf.math.divide(attn_score, scale)\n","        # do mask (subtract 1e-9 for masked value)\n","        attn_scale -= 1.e9 * attn_mask\n","        # calculate attention prob\n","        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n","        # weighted sum of V\n","        attn_out = tf.matmul(attn_prob, V)\n","        return attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlFeIqeUAlOd"},"source":["# Encoder Self Attetnion\n","Q = hidden_enc\n","K = hidden_enc\n","V = hidden_enc\n","\n","attention = ScaleDotProductAttention()\n","attn_out = attention((Q, K, V, enc_self_mask))\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pU-46iGWeIM3"},"source":["# Decoder Self Attetnion\n","Q = hidden_dec\n","K = hidden_dec\n","V = hidden_dec\n","\n","attn_out = attention((Q, K, V, dec_self_mask))\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxxgRGhJfdAj"},"source":["# Encoder-Decoder Attetnion\n","Q = hidden_dec\n","K = hidden_enc\n","V = hidden_enc\n","\n","attn_out = attention((Q, K, V, enc_dec_mask))\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y6_0vG8SISFB"},"source":["# Multi Head Attention"]},{"cell_type":"code","metadata":{"id":"3dkSuTsQXJTZ"},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    \"\"\"\n","    Multi Head Attention Class\n","    \"\"\"\n","    def __init__(self, config, name=\"multi_head_attention\"):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.d_model = config.d_model\n","        self.n_head = config.n_head\n","        self.d_head = config.d_head\n","\n","        # Q, K, V input dense layer\n","        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head)\n","        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head)\n","        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head)\n","        # Scale Dot Product Attention class\n","        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n","        # output dense layer\n","        self.W_O = tf.keras.layers.Dense(config.d_model)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: Q, K, V, attn_mask tuple\n","        :return attn_out: attention 실행 결과\n","        \"\"\"\n","        Q, K, V, attn_mask = inputs\n","        # build multihead Q, K, V\n","        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [-1, tf.shape(Q)[1], self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n","        K_m = tf.transpose(tf.reshape(self.W_K(K), [-1, tf.shape(K)[1], self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n","        V_m = tf.transpose(tf.reshape(self.W_V(V), [-1, tf.shape(V)[1], self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n","        # build multihead mask\n","        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n","        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n","        attn_out_m = self.attention((Q_m, K_m, V_m, attn_mask_m))  # (bs, n_head, Q_len, d_head)\n","        # transpose and reshape\n","        attn_out_t = tf.transpose(attn_out_m, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n","        attn_out_c = tf.reshape(attn_out_t, [-1, tf.shape(Q)[1], config.n_head * config.d_head])  # (bs, Q_len, d_model)\n","        # linear for output\n","        attn_out = self.W_O(attn_out_c) # (bs, Q_len, d_model)\n","        return attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iYJFgcWCzjka"},"source":["# Encoder Self Attetnion\n","Q = hidden_enc\n","K = hidden_enc\n","V = hidden_enc\n","\n","attention = MultiHeadAttention(config)\n","attn_out = attention((Q, K, V, enc_self_mask))\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sD7ckaZWznSu"},"source":["# Decoder Self Attetnion\n","Q = hidden_dec\n","K = hidden_dec\n","V = hidden_dec\n","\n","attn_out = attention((Q, K, V, dec_self_mask))\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vM9SlWCTzuI0"},"source":["# Encoder-Decoder Attetnion\n","Q = hidden_dec\n","K = hidden_enc\n","V = hidden_enc\n","\n","attn_out = attention((Q, K, V, enc_dec_mask))\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J5TmKt9JIXzc"},"source":["# Feed Forward"]},{"cell_type":"code","metadata":{"id":"1MPtv3zPbxVm"},"source":["class PositionWiseFeedForward(tf.keras.layers.Layer):\n","    \"\"\"\n","    Position Wise Feed Forward Class\n","    \"\"\"\n","    def __init__(self, config, name=\"feed_forward\"):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=tf.nn.relu)\n","        self.W_2 = tf.keras.layers.Dense(config.d_model)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: inputs\n","        :return ff_val: feed forward 실행 결과\n","        \"\"\"\n","        # linear W_1 and W_2\n","        ff_val = self.W_1(inputs)\n","        ff_val = self.W_2(ff_val)\n","        return ff_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQdqftXNb1al"},"source":["# feed-forward class 동작 확인\n","feed_forward = PositionWiseFeedForward(config)\n","ff_val = feed_forward(hidden_enc)\n","ff_val.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uuav6G6Bh8L4"},"source":["# LayerNormal\n","- https://arxiv.org/abs/1607.06450"]},{"cell_type":"code","metadata":{"id":"mzW12eb4h_-4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612744964942,"user_tz":-540,"elapsed":595,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}},"outputId":"c00fd7f5-0a49-4bc0-c575-2247e5300b41"},"source":["# 큰 hidden 생성\n","hidden = np.array([[1, 2, 3],\n","                   [11, 11, 13],\n","                   [111, 122, 133]]).astype(np.float32)\n","hidden"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  1.,   2.,   3.],\n","       [ 11.,  11.,  13.],\n","       [111., 122., 133.]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"MX2ETxuBiVIy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612744966249,"user_tz":-540,"elapsed":588,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}},"outputId":"f7aa7888-74b1-4498-f900-623c88254a9f"},"source":["# layer_normal 실행\n","layer_norm = tf.keras.layers.LayerNormalization()\n","layer_norm(hidden)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n","array([[-1.2238274 ,  0.        ,  1.2238274 ],\n","       [-0.70670974, -0.70670974,  1.4134184 ],\n","       [-1.2247372 ,  0.        ,  1.2247372 ]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"kDXDxzWjijuo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612744968312,"user_tz":-540,"elapsed":568,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}},"outputId":"a35141f0-fe66-4879-859f-f7772be89206"},"source":["# weights\n","layer_norm.get_weights()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([1., 1., 1.], dtype=float32), array([0., 0., 0.], dtype=float32)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"C8TEhPFuiqcj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612744970146,"user_tz":-540,"elapsed":587,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}},"outputId":"a85c91d9-8607-4280-da50-adbe41c8d4a9"},"source":["# 평균 값\n","mean = np.mean(hidden, axis=-1, keepdims=True)\n","mean"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  2.      ],\n","       [ 11.666667],\n","       [122.      ]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"YD3GOLr1lV_X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612744971390,"user_tz":-540,"elapsed":572,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}},"outputId":"6a6ad585-bd63-4487-be6e-41ae8b04f550"},"source":["# sqrt(var - epsiolo)\n","sigma = np.sqrt(np.var(hidden, axis=-1, keepdims=True) + 0.001)\n","sigma"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.81710875],\n","       [0.9433392 ],\n","       [8.981518  ]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"RLXZDyCAjAI4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612744972693,"user_tz":-540,"elapsed":584,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}},"outputId":"323a6678-167a-40de-bd93-d8253bcc1aea"},"source":["# layer normal 계산\n","(hidden - mean) / sigma"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-1.2238274,  0.       ,  1.2238274],\n","       [-0.7067097, -0.7067097,  1.4134184],\n","       [-1.2247373,  0.       ,  1.2247373]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"ObWGnfj6Ic_M"},"source":["# Encoder Layer"]},{"cell_type":"code","metadata":{"id":"TOJHTsjinOry"},"source":["class EncoderLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    Encoder Layer Class\n","    \"\"\"\n","    def __init__(self, config, name='encoder_layer'):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.self_attention = MultiHeadAttention(config)\n","        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.norm_eps)\n","\n","        self.ffn = PositionWiseFeedForward(config)\n","        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.norm_eps)\n","\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n"," \n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: enc_hidden, self_mask tuple\n","        :return enc_out: EncoderLayer 실행 결과\n","        \"\"\"\n","        enc_hidden, self_mask = inputs\n","        # self attention\n","        self_attn_val = self.self_attention((enc_hidden, enc_hidden, enc_hidden, self_mask))\n","        # add and layer normal\n","        norm1_val = self.norm1(enc_hidden + self.dropout(self_attn_val))\n","        \n","        # feed forward\n","        ffn_val = self.ffn(norm1_val)\n","        # add and layer normal\n","        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n","\n","        return enc_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"heThi4mtoGhW"},"source":["# EncoderLayer 기능 확인\n","encoder_layer = EncoderLayer(config)\n","enc_out = encoder_layer((hidden_enc, enc_self_mask))\n","enc_out.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MH_M42l5Iezo"},"source":["# Decoder Layer"]},{"cell_type":"code","metadata":{"id":"O2ZJaGlqsxI6"},"source":["class DecoderLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    Decoder Layer Class\n","    \"\"\"\n","    def __init__(self, config, name='decoder_layer'):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.self_attention = MultiHeadAttention(config)\n","        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.norm_eps)\n","\n","        self.ende_attn = MultiHeadAttention(config)\n","        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.norm_eps)\n","\n","        self.ffn = PositionWiseFeedForward(config)\n","        self.norm3 = tf.keras.layers.LayerNormalization(epsilon=config.norm_eps)\n","\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: dec_hidden, enc_out, self_mask, ende_mask tuple\n","        :return dec_out: DecoderLayer 실행 결과\n","        \"\"\"\n","        dec_hidden, enc_out, self_mask, ende_mask = inputs\n","        # self attention\n","        self_attn_val = self.self_attention((dec_hidden, dec_hidden, dec_hidden, self_mask))\n","        # add and layer normal\n","        norm1_val = self.norm1(dec_hidden + self.dropout(self_attn_val))\n","\n","        # encoder and decoder attention\n","        ende_attn_val = self.ende_attn((norm1_val, enc_out, enc_out, ende_mask))\n","        # add and layer normal\n","        norm2_val = self.norm2(norm1_val + self.dropout(ende_attn_val))\n","\n","        # feed forward\n","        ffn_val = self.ffn(norm2_val)\n","        # add and layer normal\n","        dec_out = self.norm3(norm2_val + self.dropout(ffn_val))\n","\n","        return dec_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TEAxwUw9tGDk"},"source":["# Decoder 실행\n","decoder_layer = DecoderLayer(config)\n","dec_out = decoder_layer((hidden_dec, hidden_enc, dec_self_mask, enc_dec_mask))\n","dec_out.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z4OCTlKPIh0Z"},"source":["# Weight Shared Embedding"]},{"cell_type":"code","metadata":{"id":"mjoffNaW4J_W"},"source":["class SharedEmbedding(tf.keras.layers.Layer):\n","    \"\"\"\n","    Weighed Shaed Embedding Class\n","    \"\"\"\n","    def __init__(self, config, name='weight_shared_embedding'):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.n_vocab = config.n_vocab\n","        self.d_model = config.d_model\n","    \n","    def build(self, input_shape):\n","        \"\"\"\n","        shared weight 생성\n","        :param input_shape: Tensor Shape (not used)\n","        \"\"\"\n","        with tf.name_scope('shared_embedding_weight'):\n","            self.shared_weights = self.add_weight(\n","                'weights',\n","                shape=[self.n_vocab, self.d_model],\n","                initializer=tf.keras.initializers.TruncatedNormal(stddev=self.d_model ** -0.5)\n","            )\n","\n","    def call(self, inputs, mode='embedding'):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: 입력\n","        :param mode: 실행 모드\n","        :return: embedding or linear 실행 결과\n","        \"\"\"\n","        # mode가 embedding일 경우 embedding lookup 실행\n","        if mode == 'embedding':\n","            return self._embedding(inputs)\n","        # mode가 linear일 경우 linear 실행\n","        elif mode == 'linear':\n","            return self._linear(inputs)\n","        # mode가 기타일 경우 오류 발생\n","        else:\n","            raise ValueError(f'mode {mode} is not valid.')\n","    \n","    def _embedding(self, inputs):\n","        \"\"\"\n","        embedding lookup\n","        :param inputs: 입력\n","        \"\"\"\n","        # lookup by gather\n","        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n","        # muliply d_model ** 0.5\n","        embed *= self.d_model ** 0.5\n","        return embed\n","\n","    def _linear(self, inputs):  # (bs, n_seq, d_model)\n","        \"\"\"\n","        linear 실행\n","        :param inputs: 입력\n","        \"\"\"\n","        # matmul inputs, shared_weights (transpose_b=True)\n","        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kFRtg0U7o5W"},"source":["embedding = SharedEmbedding(config)\n","hidden_dec = embedding(train_dec_inputs)\n","hidden_dec.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttHI4hYM7os_"},"source":["linear_outputs = embedding(hidden_dec, mode=\"linear\")\n","linear_outputs.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zv8egauKIl6h"},"source":["# Postional Encoding"]},{"cell_type":"code","metadata":{"id":"IfIQaEslDFPP"},"source":["class PositionalEmbedding(tf.keras.layers.Layer):\n","    \"\"\"\n","    Positional Embedding Class\n","    \"\"\"\n","    def __init__(self, config, name='position_embedding'):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","        \n","        pos_encoding = PositionalEmbedding.get_sinusoid_encoding(config.n_seq, config.d_model)\n","        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, trainable=False, weights=[pos_encoding])\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: 입력\n","        :return embed: positional embedding lookup 결과\n","        \"\"\"\n","        # make position (0...n_seq)\n","        position = tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True)\n","        position = tf.cast(position, tf.int32)\n","        # embedding lookup\n","        embed = self.embedding(position)\n","        return embed\n","\n","    @staticmethod\n","    def get_sinusoid_encoding(n_seq, d_model):\n","        \"\"\"\n","        sinusoid encoding 생성\n","        :param n_seq: sequence number\n","        :param n_seq: model hidden dimension\n","        :return: positional encoding table\n","        \"\"\"\n","        # calculate angle\n","        exs = [2 * (i_ang // 2) / d_model for i_ang in range(d_model)]\n","        angles = [np.power(10000, ex) for ex in exs]\n","        # calculate position\n","        pos_encoding = np.array([[pos / angle for angle in angles] for pos in range(n_seq)])\n","        # sin even number\n","        pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n","        # cos odd number\n","        pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n","        return tf.cast(pos_encoding, tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9P_eFKxH26ei"},"source":["# position encoding 확인\n","pos_encoding = PositionalEmbedding.get_sinusoid_encoding(4, 4)\n","pos_encoding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDMcqbPp3ipc"},"source":["# display\n","plt.pcolormesh(pos_encoding, cmap='RdBu')\n","plt.xlabel('Depth')\n","plt.xlim((0, config.d_model))\n","plt.ylabel('Position')\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5uuUYZeeDWjc"},"source":["# PositionalEmbedding 클래스 시험\n","pos_embedding = PositionalEmbedding(config)\n","dec_pos = pos_embedding(train_enc_inputs)\n","dec_pos.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yzu-44yIEfEO"},"source":["# 512x512 position encoding table 생성\n","pos_encoding = PositionalEmbedding.get_sinusoid_encoding(512, 512)\n","# display\n","plt.pcolormesh(pos_encoding, cmap='RdBu')\n","plt.xlabel('Depth')\n","plt.xlim((0, 512))\n","plt.ylabel('Position')\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sgmPfzaBIo_V"},"source":["# Transformer"]},{"cell_type":"code","metadata":{"id":"NjUj8dKbLBPV"},"source":["class Transformer(tf.keras.Model):\n","    \"\"\"\n","    Transformer Class\n","    \"\"\"\n","    def __init__(self, config, name='transformer'):\n","        \"\"\"\n","        생성자\n","        :param config: Config 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.i_pad = config.i_pad\n","        self.embedding = SharedEmbedding(config)\n","        self.position = PositionalEmbedding(config)\n","        \n","        self.encoder_layers = [EncoderLayer(config, name=f'encoder_layer_{i}') for i in range(config.n_layer)]\n","        self.decoder_layers = [DecoderLayer(config, name=f'decoder_layer_{i}') for i in range(config.n_layer)]\n","\n","        self.dropout = tf.keras.layers.Dropout(config.dropout)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: enc_tokens, dec_tokens tuple\n","        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n","        \"\"\"\n","        enc_tokens, dec_tokens = inputs\n","        # encoder self attention mask\n","        enc_self_mask = get_pad_mask(enc_tokens, self.i_pad)\n","        # decoder self attention mask\n","        dec_self_mask = get_causal_mask(dec_tokens, self.i_pad)\n","        # encoder and decoder attention mask\n","        enc_dec_mask = get_pad_mask(enc_tokens, self.i_pad)\n","\n","        # enc_tokens, dec_tokens embedding lookup\n","        enc_embed = self.get_embedding(enc_tokens)\n","        dec_embed = self.get_embedding(dec_tokens)\n","\n","        # dropout for enc_embed\n","        enc_hidden = self.dropout(enc_embed)\n","        # call encoder layers\n","        for encoder_layer in self.encoder_layers:\n","            enc_hidden = encoder_layer((enc_hidden, enc_self_mask))\n","        \n","        # dropout for dec_embed\n","        dec_hidden = self.dropout(dec_embed)\n","        # call decoder layers\n","        for decoder_layer in self.decoder_layers:\n","            dec_hidden = decoder_layer((dec_hidden, enc_hidden, dec_self_mask, enc_dec_mask))\n","\n","        # call weight shared embedding (model=linear)\n","        logits = self.embedding(dec_hidden, mode='linear')\n","        return logits\n","    \n","    def get_embedding(self, tokens):\n","        \"\"\"\n","        token embedding, position embedding lookup\n","        :param tokens: 입력 tokens\n","        :return embed: embedding 결과\n","        \"\"\"\n","        embed = self.embedding(tokens) + self.position(tokens)\n","        return embed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbcZGCC2L-MB"},"source":["# Transformer 기능 확인. 최종 결과가 (bs, n_seq(dec), n_vocab)\n","transformer = Transformer(config)\n","logits = transformer((train_enc_inputs, train_dec_inputs))\n","logits.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lGaoO7ckM0XM"},"source":[""],"execution_count":null,"outputs":[]}]}